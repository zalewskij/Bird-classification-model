{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68726db873b8dce7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classes and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from birdclassification.preprocessing.filtering import filter_recordings_30\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "import random\n",
    "from birdclassification.preprocessing.augmentations_wrappers import InvertPolarity, AddWhiteNoise, PitchShifting, RandomGain, TimeShift, RandomChunk\n",
    "from birdclassification.preprocessing.spectrogram import generate_mel_spectrogram\n",
    "from birdclassification.preprocessing.utils import get_loudest_index, cut_around_index\n",
    "from birdclassification.visualization.plots import plot_torch_spectrogram"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:04.669230542Z",
     "start_time": "2023-11-13T21:50:04.562667566Z"
    }
   },
   "id": "1e1e43838f725aab"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    \n",
    "SEED = 123\n",
    "RECORDINGS_DIR = '/media/jacek/E753-A120/recordings_30/'\n",
    "SAMPLE_RATE = 32000\n",
    "NUM_SAMPLES = SAMPLE_RATE * 1\n",
    "BATCH_SIZE = 64\n",
    "random.seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:04.758433858Z",
     "start_time": "2023-11-13T21:50:04.569783063Z"
    }
   },
   "id": "816e75a17dc17a2"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class AugmentationsPipeline(torch.nn.Module):\n",
    "    def __init__(self, target_sr = 32000, n_fft = 512, hop_length = 3 * 128, sample_length = 3, number_of_bands = 64, fmin = 150, fmax = 15000):\n",
    "        super().__init__()\n",
    "        self.target_sr = target_sr\n",
    "        self.augmentations = [InvertPolarity(), \n",
    "                              AddWhiteNoise(min_factor=0.1, max_factor=0.8), \n",
    "                              RandomGain(min_factor=0.5, max_factor=1.5), \n",
    "                              TimeShift(min_factor=0.1, max_factor=0.3), \n",
    "                              RandomChunk(sr = target_sr, min_factor=0.1 , max_factor=1), \n",
    "                              PitchShifting(sr = target_sr, min_semitones=1, max_semitones=10)]\n",
    "        \n",
    "        self.augmentations = []\n",
    "        self.probabilities = [0.5 for i in range(len(self.augmentations))]\n",
    "        self.get_spectrogram = generate_mel_spectrogram\n",
    "        self.get_loudest_index = get_loudest_index\n",
    "        self.cut_around_largest_index = cut_around_index\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.sample_length = sample_length\n",
    "        self.number_of_bands = number_of_bands\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor: \n",
    "        #select loudest 3 second chunk\n",
    "        peak = get_loudest_index(waveform, self.n_fft, self.hop_length)\n",
    "        waveform = cut_around_index(waveform, peak, self.target_sr * self.sample_length)\n",
    "        #augmentations\n",
    "        #n = random.randint(0, len(self.augmentations))\n",
    "        #selected = random.choices(list(self.augmentations), weights=self.probabilities, k=n)\n",
    "        #print(selected)\n",
    "        #aug = torch.nn.Sequential(*selected)\n",
    "        #augmented = aug(waveform)\n",
    "        waveform = self.mix_down_if_necessary(waveform)\n",
    "        #generate spectrogram\n",
    "        spectrogram = self.get_spectrogram(waveform, self.target_sr, self.n_fft, self.hop_length)\n",
    "        \n",
    "        return spectrogram\n",
    "    \n",
    "    def mix_down_if_necessary(self, audio):\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = torch.mean(audio, dim = 0,  keepdim=True)\n",
    "        return audio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:04.758876330Z",
     "start_time": "2023-11-13T21:50:04.615381293Z"
    }
   },
   "id": "db91ea0c59dadf45"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class Recordings30(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 recording_dir,\n",
    "                 transform=None,\n",
    "                 sample_rate=32000,\n",
    "                 device = \"cpu\"):\n",
    "        \n",
    "        df['filepath'] = df.apply(lambda x: f\"{recording_dir}{x['Latin name']}/{str(x['id'])}.mp3\" , axis=1)\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['Latin name'])\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.filepath = df['filepath'].to_numpy()\n",
    "        self.label = df['label'].to_numpy()\n",
    "        self.device = device\n",
    "        self.recording_dir = recording_dir\n",
    "        self.augmentation_pipeline = AugmentationsPipeline()\n",
    "        #self.transform = transform.to(self.device) \n",
    "        self.le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.filepath.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, sr = torchaudio.load(self.filepath[idx])\n",
    "        print(f\"After load: {audio.shape}\")\n",
    "        \n",
    "        #select fragment\n",
    "        #audio = audio.to(self.device)\n",
    "        label = self.label[idx]\n",
    "\n",
    "        audio = self.augmentation_pipeline(audio)\n",
    "        print(f\"Shape after augmentation pipeline: {audio.shape}\")\n",
    "\n",
    "        # if self.transform:\n",
    "        #     audio = self.transform(audio)\n",
    "        return audio, label\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:04.759113324Z",
     "start_time": "2023-11-13T21:50:04.615525036Z"
    }
   },
   "id": "195d2982c1dd616d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset and dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2500b5a9a57f175"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacek/PycharmProjects/Bird-classification-model/birdclassification/preprocessing/filtering.py:45: DtypeWarning: Columns (10,39,43,44,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  recordings = pd.read_csv(filepath_recordings)\n"
     ]
    }
   ],
   "source": [
    "df = filter_recordings_30(\"../../data/xeno_canto_recordings.csv\", \"../../data/bird-list-extended.csv\", )\n",
    "\n",
    "train_df, test_val_df = train_test_split(df, stratify=df['Latin name'], test_size=0.2, random_state = SEED)\n",
    "val_df, test_df = train_test_split(test_val_df, stratify=test_val_df['Latin name'], test_size=0.5, random_state = SEED)\n",
    "\n",
    "train_ds = Recordings30(train_df, recording_dir=RECORDINGS_DIR, sample_rate=SAMPLE_RATE, device = DEVICE)\n",
    "val_ds = Recordings30(val_df, recording_dir=RECORDINGS_DIR, device = DEVICE)\n",
    "test_ds = Recordings30(test_df, recording_dir=RECORDINGS_DIR, device = DEVICE)\n",
    "\n",
    "train_dl  = DataLoader(train_ds, batch_size= BATCH_SIZE)\n",
    "val_dl  = DataLoader(val_ds, batch_size= BATCH_SIZE)\n",
    "test_dl  = DataLoader(test_ds, batch_size= BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:11.392962772Z",
     "start_time": "2023-11-13T21:50:04.625383888Z"
    }
   },
   "id": "63a6e7eb0ea22a95"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"/media/jacek/E753-A120/recordings_30/Lophophanes cristatus/505283.mp3\" (No such file or directory).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m audio, label \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_ds\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(label)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(audio\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[0;32mIn[41], line 26\u001B[0m, in \u001B[0;36mRecordings30.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m---> 26\u001B[0m     audio, sr \u001B[38;5;241m=\u001B[39m \u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAfter load: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maudio\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m#select fragment\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m#audio = audio.to(self.device)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Bird-classification-model/venv/lib/python3.11/site-packages/torchaudio/_backend/utils.py:203\u001B[0m, in \u001B[0;36mget_load_func.<locals>.load\u001B[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load audio data from source.\u001B[39;00m\n\u001B[1;32m    127\u001B[0m \n\u001B[1;32m    128\u001B[0m \u001B[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    202\u001B[0m backend \u001B[38;5;241m=\u001B[39m dispatcher(uri, \u001B[38;5;28mformat\u001B[39m, backend)\n\u001B[0;32m--> 203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Bird-classification-model/venv/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:334\u001B[0m, in \u001B[0;36mFFmpegBackend.load\u001B[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001B[0m\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_audio_fileobj(\n\u001B[1;32m    325\u001B[0m         uri,\n\u001B[1;32m    326\u001B[0m         frame_offset,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    331\u001B[0m         buffer_size,\n\u001B[1;32m    332\u001B[0m     )\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormpath\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Bird-classification-model/venv/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:100\u001B[0m, in \u001B[0;36mload_audio\u001B[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_audio\u001B[39m(\n\u001B[1;32m     92\u001B[0m     src: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m     93\u001B[0m     frame_offset: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mformat\u001B[39m: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     98\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28mfilter\u001B[39m \u001B[38;5;241m=\u001B[39m _get_load_filter(frame_offset, num_frames, convert)\n\u001B[0;32m--> 100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompat_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Bird-classification-model/venv/lib/python3.11/site-packages/torch/_ops.py:692\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    689\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    690\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    691\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to open the input \"/media/jacek/E753-A120/recordings_30/Lophophanes cristatus/505283.mp3\" (No such file or directory)."
     ]
    }
   ],
   "source": [
    "audio, label = train_ds[0]\n",
    "print(label)\n",
    "print(audio.shape)\n",
    "plot_torch_spectrogram(audio)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T09:41:46.949464476Z",
     "start_time": "2023-11-14T09:41:46.612174552Z"
    }
   },
   "id": "4fa92856fc444764"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "268104dedd5e98ad"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "571f6af9acb5613b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:12.043000958Z",
     "start_time": "2023-11-13T21:50:12.034449561Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / liniear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(\n",
    "            10880, 30 \n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "            \n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        print(\"input\", input_data.shape)\n",
    "        x = self.conv1(input_data)\n",
    "        print(\"conv1\", x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(\"conv2\", x.shape)\n",
    "        x = self.conv3(x)\n",
    "        print(\"conv3\", x.shape)\n",
    "        x = self.conv4(x)\n",
    "        print(\"conv4\", x.shape)\n",
    "        x = self.flatten(x)\n",
    "        print(\"flatten\", x.shape)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:30.014177991Z",
     "start_time": "2023-11-13T21:50:29.934863874Z"
    }
   },
   "id": "cbcc42544838e223"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([2, 1, 64, 251])\n",
      "conv1 torch.Size([2, 16, 33, 126])\n",
      "conv2 torch.Size([2, 32, 17, 64])\n",
      "conv3 torch.Size([2, 64, 9, 33])\n",
      "conv4 torch.Size([2, 128, 5, 17])\n",
      "flatten torch.Size([2, 10880])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 66, 253]             160\n",
      "              ReLU-2          [-1, 16, 66, 253]               0\n",
      "         MaxPool2d-3          [-1, 16, 33, 126]               0\n",
      "            Conv2d-4          [-1, 32, 35, 128]           4,640\n",
      "              ReLU-5          [-1, 32, 35, 128]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 64]               0\n",
      "            Conv2d-7           [-1, 64, 19, 66]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 66]               0\n",
      "         MaxPool2d-9            [-1, 64, 9, 33]               0\n",
      "           Conv2d-10          [-1, 128, 11, 35]          73,856\n",
      "             ReLU-11          [-1, 128, 11, 35]               0\n",
      "        MaxPool2d-12           [-1, 128, 5, 17]               0\n",
      "          Flatten-13                [-1, 10880]               0\n",
      "           Linear-14                   [-1, 30]         326,430\n",
      "          Softmax-15                   [-1, 30]               0\n",
      "================================================================\n",
      "Total params: 423,582\n",
      "Trainable params: 423,582\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 9.33\n",
      "Params size (MB): 1.62\n",
      "Estimated Total Size (MB): 11.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn = CNNNetwork()\n",
    "summary(cnn, (1, 64, 251)) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:32.905906452Z",
     "start_time": "2023-11-13T21:50:32.885634721Z"
    }
   },
   "id": "ce99d90c296dcb74"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "cnn = CNNNetwork().to(device)\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:40.630297420Z",
     "start_time": "2023-11-13T21:50:40.617557968Z"
    }
   },
   "id": "bb4278a15dea907"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "#     for inputs, targets in data_loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "# \n",
    "#         #calculate loss\n",
    "#         predictions = model(inputs)\n",
    "#         loss = loss_fn(predictions, targets)\n",
    "# \n",
    "#         #backpropagate loss and update weights\n",
    "#         optimiser.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimiser.step()\n",
    "# \n",
    "#     print(f\"Loss {loss.item()}\")\n",
    "# \n",
    "# \n",
    "# def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
    "#     for i in range(epochs):\n",
    "#         print(f\"Epoch {i + 1}\")\n",
    "#         train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
    "#     print(\"Finished training\")\n",
    "# \n",
    "# \n",
    "# def predict(model, input, target, class_mapping):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predctions = model(input)\n",
    "#         predicted_index = predctions[0].argmax(0)\n",
    "#         predicted = class_mapping[predicted_index]\n",
    "#         expected = class_mapping[target]\n",
    "#     return predicted, expected"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:50:43.188320620Z",
     "start_time": "2023-11-13T21:50:43.183634884Z"
    }
   },
   "id": "af6db4b50b693d7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def train_one_epoch(epoch_index, tb_writer, dataloader):\n",
    "#     running_loss = 0.\n",
    "#     last_loss = 0.\n",
    "# \n",
    "#     # Here, we use enumerate(training_loader) instead of\n",
    "#     # iter(training_loader) so that we can track the batch\n",
    "#     # index and do some intra-epoch reporting\n",
    "#     for i, data in enumerate(dataloader):\n",
    "#         # Every data instance is an input + label pair\n",
    "#         inputs, labels = data\n",
    "# \n",
    "#         # Zero your gradients for every batch!\n",
    "#         optimizer.zero_grad()\n",
    "# \n",
    "#         # Make predictions for this batch\n",
    "#         outputs = model(inputs)\n",
    "# \n",
    "#         # Compute the loss and its gradients\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss.backward()\n",
    "# \n",
    "#         # Adjust learning weights\n",
    "#         optimizer.step()\n",
    "# \n",
    "#         # Gather data and report\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 1000 == 999:\n",
    "#             last_loss = running_loss / 1000 # loss per batch\n",
    "#             print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "#             tb_x = epoch_index * len(training_loader) + i + 1\n",
    "#             tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "#             running_loss = 0.\n",
    "# \n",
    "#     return last_loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970d2be83e28f5f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EPOCHS = 1\n",
    "# LEARNING_RATE = 0.001\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimiser = torch.optim.Adam(cnn.parameters(), LEARNING_RATE)\n",
    "# train(cnn, train_dl, loss_fn, optimiser, device, EPOCHS)\n",
    "# \n",
    "# torch.save(cnn.state_dict(), \"cnn.pth\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d4d6b60baba8699"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d140231a0259701c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b10817059f8b0bb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f3a381430a4687b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "af0daaf8bd117879"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3960e6d75bedc4a5"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:22:30.908443683Z",
     "start_time": "2023-11-14T10:22:30.897740189Z"
    }
   },
   "id": "e5b5b81e5b121d65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T21:50:12.299772807Z"
    }
   },
   "id": "6bf17803c91b88f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
