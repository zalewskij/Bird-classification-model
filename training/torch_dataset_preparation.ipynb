{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Create custom dataset\n",
    "<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7136e657ea9d86a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.io\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import librosa\n",
    "from birdclassification.preprocessing.filtering import filter_recordings_30\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, df, recording_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame of xeno-canto recordings\n",
    "        recording_dir: str\n",
    "            filepath to directory with recordings\n",
    "        transform:\n",
    "        target_transform:\n",
    "        \"\"\"\n",
    "        \n",
    "        df['filepath'] = df.apply(lambda x: f\"{recording_dir}{x['Latin name']}/{str(x['id'])}.mp3\" , axis=1)\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['Latin name'])\n",
    "        \n",
    "        self.filepath = df['filepath'].to_numpy()\n",
    "        self.label = df['label'].to_numpy()\n",
    "        self.recording_dir = recording_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.filepath.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, sr = librosa.load(self.filepath[idx])\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        audio = torch.from_numpy(audio).type(torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.int8)\n",
    "        \n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return audio, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7acaf633a4d17fa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22a3d378997a174b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RECORDINGS_DIR = '/media/jacek/E753-A120/recordings_30/'\n",
    "\n",
    "df = filter_recordings_30()\n",
    "\n",
    "train_df, test_val_df = train_test_split(df, stratify=df['Latin name'], test_size=0.2)\n",
    "val_df, test_df = train_test_split(test_val_df, stratify=test_val_df['Latin name'], test_size=0.5)\n",
    "\n",
    "train_ds = CustomAudioDataset(train_df, recording_dir=RECORDINGS_DIR)\n",
    "val_ds = CustomAudioDataset(val_df, recording_dir=RECORDINGS_DIR)\n",
    "test_ds = CustomAudioDataset(test_df, recording_dir=RECORDINGS_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "84c31e7db36c83e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_ds.__len__())\n",
    "print(val_ds.__len__())\n",
    "print(test_ds.__len__())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8719c96404cbe265"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "69d3b86e77e5c382"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0d5413528425ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a1e358a1dc5031c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for element in train_dataloader:\n",
    "    print(element)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ad408f4f0a875755"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example of custom transform on audio"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f281f1f308aa0672"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio.transforms import Resample, TimeStretch, Spectrogram, FrequencyMasking, TimeMasking, MelScale, AddNoise\n",
    "from birdclassification.visualization.plots import plot_waveform\n",
    "\n",
    "class MyPipeline(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_freq=16000,\n",
    "        resample_freq=8000,\n",
    "        n_fft=1024,\n",
    "        n_mel=256,\n",
    "        stretch_factor=0.8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.resample = Resample(orig_freq=input_freq, new_freq=resample_freq)\n",
    "        self.spec = Spectrogram(n_fft=n_fft, power=2)\n",
    "        self.spec_aug = torch.nn.Sequential(\n",
    "            TimeStretch(stretch_factor, fixed_rate=True),\n",
    "            FrequencyMasking(freq_mask_param=80),\n",
    "            TimeMasking(time_mask_param=80),\n",
    "            AddNoise()\n",
    "        )\n",
    "        self.mel_scale = MelScale(\n",
    "            n_mels=n_mel, sample_rate=resample_freq, n_stft=n_fft // 2 + 1)\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        # Resample the input\n",
    "        resampled = self.resample(waveform)\n",
    "\n",
    "        # # Convert to power spectrogram\n",
    "        # spec = self.spec(resampled)\n",
    "        # \n",
    "        # # Apply SpecAugment\n",
    "        # spec = self.spec_aug(spec)\n",
    "        # \n",
    "        # # Convert to mel-scale\n",
    "        # mel = self.mel_scale(spec)\n",
    "        return resampled"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "44a6e4e1384b4d13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate a pipeline\n",
    "pipeline = MyPipeline()\n",
    "# # Move the computation graph to CUDA\n",
    "# pipeline.to(device=torch.device(\"cuda\"), dtype=torch.float32)\n",
    "audio, label = train_ds[3]\n",
    "plot_waveform(audio, 20000)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "44997de9bdea54de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = pipeline(audio)\n",
    "plot_waveform()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "42c9e665c923fc9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
