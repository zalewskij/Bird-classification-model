{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Create custom dataset\n",
    "<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7136e657ea9d86a7"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchvision.io\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import librosa\n",
    "from birdclassification.preprocessing.filtering import filter_recordings_30\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, df, recording_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame of xeno-canto recordings\n",
    "        recording_dir: str\n",
    "            filepath to directory with recordings\n",
    "        transform:\n",
    "        target_transform:\n",
    "        \"\"\"\n",
    "        \n",
    "        df['filepath'] = df.apply(lambda x: f\"{recording_dir}{x['Latin name']}/{str(x['id'])}.mp3\" , axis=1)\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['Latin name'])\n",
    "        \n",
    "        self.filepath = df['filepath'].to_numpy()\n",
    "        self.label = df['label'].to_numpy()\n",
    "        self.recording_dir = recording_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.filepath.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, sr = torchaudio.load(self.filepath[idx])\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        audio = torch.from_numpy(audio).type(torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.int8)\n",
    "        \n",
    "        # augmented = pipeline(audio, sr)\n",
    "        \n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)       \n",
    "        \n",
    "        return audio, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:53:57.839253Z",
     "start_time": "2023-11-15T12:53:57.776980Z"
    }
   },
   "id": "7acaf633a4d17fa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22a3d378997a174b"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zosia/Desktop/Bird-classification-model/birdclassification/preprocessing/filtering.py:45: DtypeWarning: Columns (10,39,43,44,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  recordings = pd.read_csv(filepath_recordings)\n"
     ]
    }
   ],
   "source": [
    "RECORDINGS_DIR = '/Users/zosia/Desktop/recordings_30/'\n",
    "\n",
    "df = filter_recordings_30()\n",
    "\n",
    "train_df, test_val_df = train_test_split(df, stratify=df['Latin name'], test_size=0.2)\n",
    "val_df, test_df = train_test_split(test_val_df, stratify=test_val_df['Latin name'], test_size=0.5)\n",
    "\n",
    "train_ds = CustomAudioDataset(train_df, recording_dir=RECORDINGS_DIR)\n",
    "val_ds = CustomAudioDataset(val_df, recording_dir=RECORDINGS_DIR)\n",
    "test_ds = CustomAudioDataset(test_df, recording_dir=RECORDINGS_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:03.821206Z",
     "start_time": "2023-11-15T12:53:57.782688Z"
    }
   },
   "id": "84c31e7db36c83e2"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32968\n",
      "4121\n",
      "4121\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.__len__())\n",
    "print(val_ds.__len__())\n",
    "print(test_ds.__len__())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:03.829131Z",
     "start_time": "2023-11-15T12:54:03.821895Z"
    }
   },
   "id": "8719c96404cbe265"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     print(train_ds[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:22.948016Z",
     "start_time": "2023-11-15T12:54:22.931211Z"
    }
   },
   "id": "69d3b86e77e5c382"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0d5413528425ca"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:24.221855Z",
     "start_time": "2023-11-15T12:54:24.213543Z"
    }
   },
   "id": "a1e358a1dc5031c6"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# for element in train_dataloader:\n",
    "#     print(element)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:25.064207Z",
     "start_time": "2023-11-15T12:54:25.048359Z"
    }
   },
   "id": "ad408f4f0a875755"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example of custom transform on audio"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f281f1f308aa0672"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbirdclassification\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvisualization\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplots\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m plot_waveform\n\u001B[0;32m----> 2\u001B[0m audio, label \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_ds\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m      3\u001B[0m plot_waveform(audio, \u001B[38;5;241m20000\u001B[39m)\n",
      "Cell \u001B[0;32mIn[29], line 42\u001B[0m, in \u001B[0;36mCustomAudioDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     39\u001B[0m audio, sr \u001B[38;5;241m=\u001B[39m torchaudio\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath[idx])\n\u001B[1;32m     40\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel[idx]\n\u001B[0;32m---> 42\u001B[0m audio \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     43\u001B[0m label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(label, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint8)\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# augmented = pipeline(audio, sr)\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "from birdclassification.visualization.plots import plot_waveform\n",
    "audio, label = train_ds[3]\n",
    "plot_waveform(audio, 20000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:26.238015Z",
     "start_time": "2023-11-15T12:54:26.153365Z"
    }
   },
   "id": "b5fa309eff3986c2"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from birdclassification.preprocessing.spectrogram import generate_mel_spectrogram\n",
    "from birdclassification.preprocessing.utils import get_loudest_index, cut_around_index\n",
    "import random\n",
    "from torchaudio.transforms import Resample\n",
    "from birdclassification.preprocessing.augmentations_wrappers import InvertPolarity, AddWhiteNoise, PitchShifting, RandomGain, TimeShift, RandomChunk, TimeStretch\n",
    "\n",
    "class AugmentationsPipeline(torch.nn.Module):\n",
    "    def __init__(self, target_sr = 32000, n_fft = 512, hop_length = 3 * 128, sample_length = 3, number_of_bands = 64, fmin = 150, fmax = 15000):\n",
    "        super().__init__()\n",
    "        self.target_sr = target_sr\n",
    "        self.augmentations = [InvertPolarity(), \n",
    "                              AddWhiteNoise(min_factor=0.1, max_factor=0.8), \n",
    "                              RandomGain(min_factor=0.5, max_factor=1.5), \n",
    "                              TimeShift(min_factor=0.1, max_factor=0.3), \n",
    "                              RandomChunk(sr = target_sr, min_factor=0.1 , max_factor=1), \n",
    "                              PitchShifting(sr = target_sr, min_semitones=1, max_semitones=10)]\n",
    "        \n",
    "        self.probabilities = [0.5 for i in range(len(self.augmentations))]\n",
    "        self.get_spectrogram = generate_mel_spectrogram\n",
    "        self.get_loudest_index = get_loudest_index\n",
    "        self.cut_around_largest_index = cut_around_index\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.sample_length = sample_length\n",
    "        self.number_of_bands = number_of_bands\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        \n",
    "        \n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor: \n",
    "        #select loudest 3 second chunk\n",
    "        peak = get_loudest_index(waveform, self.n_fft, self.hop_length)\n",
    "        waveform = cut_around_index(waveform, peak, self.target_sr * self.sample_length)\n",
    "        \n",
    "        #augmentations\n",
    "        if self.augmentations:\n",
    "            n = random.randint(0, len(self.augmentations))\n",
    "            selected = random.choices(list(self.augmentations), weights=self.probabilities, k=n)\n",
    "            print(selected)\n",
    "            aug = torch.nn.Sequential(*selected)\n",
    "            waveform = aug(waveform)\n",
    "        \n",
    "        waveform = self.mix_down_if_necessary(waveform)\n",
    "        \n",
    "        #generate spectrogram\n",
    "        spectrogram = self.get_spectrogram(waveform, self.target_sr, self.n_fft, self.hop_length)\n",
    "        \n",
    "        return spectrogram\n",
    "    \n",
    "    def mix_down_if_necessary(self, audio):\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = torch.mean(audio, dim = 0,  keepdim=True)\n",
    "        return audio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:26.550923Z",
     "start_time": "2023-11-15T12:54:26.547489Z"
    }
   },
   "id": "44a6e4e1384b4d13"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m AugmentationsPipeline()\n\u001B[0;32m----> 3\u001B[0m augmented \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m plot_waveform(augmented, \u001B[38;5;241m20000\u001B[39m)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[37], line 33\u001B[0m, in \u001B[0;36mAugmentationsPipeline.forward\u001B[0;34m(self, waveform)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, waveform: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor: \n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m#select loudest 3 second chunk\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     peak \u001B[38;5;241m=\u001B[39m get_loudest_index(waveform, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_fft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhop_length)\n\u001B[0;32m---> 33\u001B[0m     waveform \u001B[38;5;241m=\u001B[39m \u001B[43mcut_around_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeak\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_sr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;66;03m#augmentations\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maugmentations:\n",
      "File \u001B[0;32m~/Desktop/Bird-classification-model/birdclassification/preprocessing/utils.py:74\u001B[0m, in \u001B[0;36mcut_around_index\u001B[0;34m(y, center, length)\u001B[0m\n\u001B[1;32m     72\u001B[0m     left_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     73\u001B[0m     right_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m half_slice_width\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m right_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m:\n\u001B[1;32m     75\u001B[0m     right_index \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     76\u001B[0m     left_index \u001B[38;5;241m=\u001B[39m right_index \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m half_slice_width\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "pipeline = AugmentationsPipeline()\n",
    "\n",
    "augmented = pipeline(audio)\n",
    "plot_waveform(augmented, 20000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:54:35.788338Z",
     "start_time": "2023-11-15T12:54:35.711890Z"
    }
   },
   "id": "42c9e665c923fc9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T12:54:03.937463Z"
    }
   },
   "id": "94c0d6dcbbd8c6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
